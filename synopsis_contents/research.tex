\section{Fabric Classification using Deep Learning}

Fabric classification is a fundamental task in the textile and fashion industries, playing a vital role in areas such as material sorting, quality control, supply chain optimization, and product design. Traditionally, fabric identification has relied on manual techniques like burn tests, chemical analysis, and microscopic inspection. While these methods can be effective, they are often time-consuming, subjective, and require expert knowledge, making them unsuitable for large-scale industrial applications.

With the rise of artificial intelligence and computer vision, deep learning has emerged as a powerful tool for automating fabric classification. Convolutional Neural Networks (CNNs) have shown strong performance in recognizing fine-grained textures and patterns in fabric images, making them effective for visual material analysis~\cite{hong2020cnn, krizhevsky2012imagenet}. On the other hand, Vision Transformers (ViTs) have recently demonstrated superior ability to model long-range dependencies and spatial relationships in images, offering complementary strengths to CNNs~\cite{dosovitskiy2020vit, chitra2022vit}.

This work explores a hybrid deep learning approach that combines CNNs and ViTs to improve the accuracy and robustness of fabric classification systems. By integrating local and global feature extraction mechanisms, the proposed model leverages the advantages of both architectures and contributes toward the development of intelligent, automated textile analysis solutions.

\section*{Literature Review}

Recent advancements in deep learning have significantly influenced the domain of fabric classification. Traditional techniques relied on physical or chemical analysis, which, while accurate, lacked scalability and efficiency. In contrast, deep learning enables automatic feature extraction from fabric images, making it suitable for industrial applications requiring speed and consistency.

Hong et al.~\cite{hong2020cnn} demonstrated the effectiveness of Convolutional Neural Networks (CNNs) in classifying clothing fabrics by fine-tuning the VGG-16 architecture. Their model achieved impressive accuracy using a controlled dataset, confirming CNNs’ suitability for texture recognition. Similarly, Krizhevsky et al.~\cite{krizhevsky2012imagenet} established the baseline performance of CNNs with their seminal work on ImageNet, laying the foundation for modern visual classification systems.

Siam et al.~\cite{siam2021textilenet} proposed a multi-model architecture using various CNN backbones, highlighting that lightweight models like MobileNetV2 can perform well on textile images. Their results emphasized the potential of transfer learning and preprocessing techniques in fabric classification tasks.

With the introduction of Vision Transformers (ViTs), researchers began leveraging self-attention mechanisms for global feature modeling. Dosovitskiy et al.~\cite{dosovitskiy2020vit} introduced the ViT architecture, which has since been applied to fabric images. Chitra et al.~\cite{chitra2022vit} combined ViTs with traditional classifiers, achieving notable performance in identifying blended fabrics, further confirming the versatility of transformer-based models.

The literature suggests that while CNNs are excellent for capturing localized features, ViTs are more effective at modeling spatial relationships. This has inspired hybrid approaches that integrate both architectures to exploit their complementary strengths for robust fabric classification.

\section*{Dataset Description}

This research utilizes two distinct datasets to address the problem of fabric classification: macro (RGB) images and Optical Coherence Tomography (OCT) images. These datasets represent complementary imaging modalities, capturing both the surface texture and the internal structure of textile fabrics.

\textbf{1. iBug Fabric Image Dataset:}  
The iBug Fabric image dataset consists of high-resolution RGB images of three commonly used fabrics: cotton, polyester, and wool. Each sample is imaged under a specially designed lighting setup that includes four different illumination angles. This allows the camera to capture subtle surface variations such as fiber alignment, weave structure, and glossiness, which are critical for discriminating between visually similar materials. The imaging setup is inspired by the micro-geometry reflectance-based acquisition system proposed by Kampouris et al.~\cite{kampouris2022microgeometry}. The camera was fixed at a working distance of 3 cm to maintain consistent focal depth across all fabric samples. Modifications to the camera optics were made to enhance image clarity and detail. Each sample consists of four images taken under different lighting conditions, increasing the dataset's variability and richness. This modality emphasizes external features such as yarn tightness, texture roughness, and reflectance characteristics.

\textbf{2. OCT Image Dataset:}  
The OCT dataset provides grayscale cross-sectional images of the same fabric types and was acquired using a swept-source Optical Coherence Tomography system as described by Sabuncu and Ozdemir~\cite{sabuncu2020oct}. This modality enables visualization of internal textile structures such as fiber density, subsurface layering, and weave depth. Unlike macro images that capture only the exterior surface, OCT allows the system to analyze the volumetric composition of fabrics, offering critical insights into their structural integrity and internal material properties.

Together, these datasets provide a comprehensive foundation for robust, multimodal fabric classification.

\section*{Methodology}

This work proposes a hybrid deep learning model that combines Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance fabric classification accuracy. The motivation behind the hybrid architecture is to leverage the local pattern recognition capabilities of CNNs and the global context modeling strengths of ViTs, thus capturing a broader range of discriminative features from fabric images.

The CNN branch consists of multiple convolutional layers with ReLU activation and max pooling, designed to extract fine-grained local textures such as weave patterns and surface irregularities. These layers reduce the spatial dimensions while preserving important textural information relevant to fabric classification.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/ModelDiagram.png}
    \caption{Proposed hybrid deep learning architecture for fabric classification.}
    \label{fig:model_diagram}
\end{figure}

In parallel, the input image is divided into fixed-size patches and passed through a ViT branch. The Vision Transformer processes these patches using multi-head self-attention and feed-forward layers, which are effective in learning long-range dependencies and global spatial relationships. Each patch is embedded into a latent space with positional encodings and passed through a series of transformer encoders.

The feature maps generated by the CNN and ViT branches are then flattened and concatenated to form a unified feature vector. This fused representation is passed through a fully connected layer, followed by a softmax classifier to predict the fabric class.

The model is trained using the cross-entropy loss function with the Adam optimizer. Standard evaluation metrics such as accuracy, precision, recall, and F1-score are used to assess performance. This hybrid architecture allows the system to generalize well across fabric types with subtle visual differences, improving robustness in real-world applications.

\section*{Results}

The proposed hybrid CNN–ViT architecture was evaluated on fabric image datasets using standard classification metrics, including accuracy, precision, recall, and F1-score. The model was tested on both macro (RGB) fabric images and Optical Coherence Tomography (OCT) cross-sectional images. Experimental implementation was carried out using PyTorch, with training optimized using the Adam optimizer and a learning rate of 0.001.

On macro images, the model achieved a classification accuracy of 96.84\% for cotton, 99.63\% for polyester, and 98.09\% for wool. The precision and recall scores were consistently high across all classes, indicating balanced performance in identifying both common and visually similar fabrics.

On the OCT dataset, the model demonstrated exceptional results, achieving 99.66\% accuracy for cotton and perfect accuracy (100\%) for polyester and wool. Precision, recall, and F1-scores were all reported as 1.00, suggesting zero misclassifications in test samples.

Comparative analysis with existing models from the literature showed that the hybrid architecture outperformed individual CNN-based approaches like VGG-16~\cite{hong2020cnn} and even lightweight CNNs such as MobileNetV2~\cite{siam2021textilenet}. It also surpassed standalone ViT-based pipelines like those proposed by Chitra et al.~\cite{chitra2022vit}, validating the effectiveness of the dual-stream fusion strategy.

Overall, the results demonstrate that combining local feature extraction (CNN) with global attention modeling (ViT) yields a more robust and accurate solution for fabric classification, capable of handling variability in texture, structure, and imaging modality.
