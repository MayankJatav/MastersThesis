\vspace{-1em}
\section{Experimental Setup and Results}

The proposed hybrid CNN–ViT architecture was implemented using the PyTorch 2.0 framework, which offers dynamic computation graphs and cross-platform GPU support, resulting in a computing environment optimized for complex deep learning models~\cite{paszke2019pytorch}. The input images were obtained from both macro (RGB) and Optical Coherence Tomography (OCT) fabric datasets and then preprocessed by normalizing their pixel intensity distributions across samples in order to maintain uniformity, and resized to a standard input size of (224×224) pixels regardless of which dataset was input into the model. All experimentation was completed on an NVIDIA DGX A100 workstation with 40GB GPU memory, with the capacity to perform evaluations and training at scale. Adam optimizer was used to train the model, as Adam was established to support stability of convergence through adaptive learning rates and momentum-based updates~\cite{kingma2014adam}. In our model, we used a learning rate of 0.001, and a batch size of 8. The learning rate and batch size were selected in a manner that allows for reasonably fast training, whilst also being capable of training on a single GPU. For the loss function, we used cross-entropy for the ability to consider a multi-class classification tasks with mutually exclusive labels (type of fabrics)~\cite{goodfellow2016deep}. Standard metrics of classification (accuracy, precision, recall, and F1 score) were used to benchmark prediction performance both as overall group classification and performance results per class.
\vspace{-1em}
\subsection{Results and Discussion}

To evaluate the effectiveness of our proposed hybrid CNN–ViT architecture, we conducted experiments on three different datasets: the Fabrics Dataset, the OCT Fabrics Dataset, and the TextileNet Dataset. Our model was compared against various methods proposed by Ying Hing et al.~\cite{hong2024research}, Chitra G M et al.~\cite{chitra2023fabric} and Siam et al.~\cite{siam2023textilenet}. The performance metrics include Accuracy, Precision, Recall, and F1-Score, along with the number of parameters in millions (M).

\begin{itemize}[topsep=0pt, noitemsep]
    \item \textbf{Fabrics Dataset:} Table~\ref{tab:rgb_results} presents the results for the RGB fabric dataset. Our model achieved 97.33\% accuracy, coming very close to the best-performing model (99.73\%) while using significantly fewer parameters (28.97M vs. 134.27M). It also outperformed all variants of Siam et al.’s models, including MobileNetV2, despite being more lightweight and efficient overall.

    \item \textbf{OCT Fabrics Dataset:} As shown in Table~\ref{tab:oct_results}, We achieved nearly perfect performance with 99.66\% accuracy and 100\% precision, recall, and F1 score. While MobileNetV2 slightly outperformed us in accuracy (99.87\%), our model still demonstrated excellent results with a balanced trade-off between accuracy and parameter size.

    \item \textbf{TextileNet Dataset:} On the more challenging TextileNet dataset, as shown in Table~\ref{tab:textilenet_results}, our model achieved 74.36\% accuracy—better than VGG16 and ConvNextBase. Although not the highest, it maintained consistent precision and recall, showing strong generalization across varied fabric types while keeping the model size efficient.
\end{itemize}

In this thesis, we proposed a hybrid deep learning architecture that integrates Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for the classification of textile fabrics using both RGB (macro) and OCT (micro) images. This design enables the model to capture localized texture-based features through CNNs and long-range spatial relationships via ViTs, providing a more comprehensive fabric representation.

\vspace{-0.90em}
\begin{table}[h]
\centering
\caption{Results on Fabrics Dataset}
\label{tab:rgb_results}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Parameters (M)} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
Ying Hing et al.~\cite{hong2024research} & 134.27 & 99.73 & 99.77 & 99.76 & 99.76 \\
Chitra G M et al.~\cite{chitra2023fabric} & 86.24 & 87.00 & 86.43 & 85.96 & 85.83 \\
Siam et al. [VGG16]~\cite{siam2023textilenet} & 27.72 & 94.12 & 94.61 & 94.17 & 94.43 \\
Siam et al. [InceptionV3]~\cite{siam2023textilenet} & 22.99 & 82.70 & 82.73 & 82.70 & 82.07 \\
Siam et al. [ResNet101]~\cite{siam2023textilenet} & 43.71 & 73.12 & 72.77 & 73.32 & 73.33 \\
Siam et al. [ConvNextBase]~\cite{siam2023textilenet} & 88.25 & 90.07 & 90.73 & 90.70 & 90.38 \\
Siam et al. [MobileNetV2]~\cite{siam2023textilenet} & 0.30 & 95.17 & 95.47 & 95.23 & 95.34 \\
\textbf{Proposed Method} & \textbf{28.97} & \textbf{97.33} & \textbf{98.00} & \textbf{97.85} & \textbf{97.60} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
\vspace{-0.90em}

We evaluated the model on three datasets: Fabrics, OCTFabrics, and TextileNet. On the Fabrics dataset, our model achieved an accuracy of 97.33\%, outperforming several baseline models and demonstrating superior performance over lightweight models such as Siam et al.'s MobileNetV2~\cite{siam2023textilenet}. For the OCTFabrics dataset, it achieved near-perfect accuracy of 99.66\% along with perfect precision, recall, and F1-score—showcasing strong discriminative power on microstructure-based data. Although MobileNetV2~\cite{siam2023textilenet} slightly surpassed our model in accuracy (99.87\%), our approach maintained significantly fewer parameters (28.97M vs.\ 42.72M).

\begin{table}[h]
\centering
\caption{Results on OCT Fabrics Dataset}
\label{tab:oct_results}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Parameters (M)} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
Ying Hing et al.~\cite{hong2024research} & 134.27 & 99.77 & 99.77 & 99.77 & 99.77 \\
Chitra G M et al.~\cite{chitra2023fabric} & 86.24 & 99.77 & 99.77 & 99.77 & 99.77 \\
Siam et al. [VGG16]~\cite{siam2023textilenet} & 27.72 & 99.69 & 100.00 & 98.74 & 98.93 \\
Siam et al. [InceptionV3]~\cite{siam2023textilenet} & 22.99 & 97.16 & 98.77 & 100.00 & 99.31 \\
Siam et al. [ResNet101]~\cite{siam2023textilenet} & 43.71 & 93.12 & 97.73 & 98.82 & 98.26 \\
Siam et al. [ConvNextBase]~\cite{siam2023textilenet} & 88.25 & 99.06 & 96.76 & 100.00 & 98.80 \\
Siam et al. [MobileNetV2]~\cite{siam2023textilenet} & 0.30 & 99.87 & 100.00 & 97.27 & 98.57 \\
\textbf{Proposed Method} & \textbf{28.97} & \textbf{99.66} & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

On the more challenging TextileNet dataset, our model achieved 74.36\% accuracy, surpassing several popular architectures such as VGG16~\cite{simonyan2015vgg} and ConvNextBase. While Ying Hing et al.~\cite{hong2024research} and Chitra G M et al.~\cite{chitra2023fabric} reported higher accuracies (88.65\% and 88.06\% respectively), our model demonstrated better parameter efficiency and balanced performance, affirming its strong generalization ability across diverse fabric types.

Overall, the fusion of CNN and ViT streams helped overcome individual limitations—CNNs excel at local feature extraction but lack global context modeling, while ViTs provide spatial awareness but benefit from CNN-based low-level representations~\cite{dosovitskiy2020vit}. This synergy resulted in a robust, accurate, and efficient architecture suitable for real-world textile classification tasks in both industrial and retail settings.

\begin{table}[h]
\centering
\caption{Results on TextileNet Dataset}
\label{tab:textilenet_results}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Parameters (M)} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
Ying Hing et al.~\cite{hong2024research} & 134.27 & 88.65 & 89.12 & 88.64 & 88.84 \\
Chitra G M et al.~\cite{chitra2023fabric} & 86.24 & 88.06 & 88.74 & 88.06 & 88.20 \\
Siam et al. [VGG16]~\cite{siam2023textilenet} & 27.72 & 24.31 & 5.91 & 24.31 & 9.52 \\
Siam et al. [InceptionV3]~\cite{siam2023textilenet} & 22.99 & 78.84 & 82.48 & 78.84 & 80.34 \\
Siam et al. [ResNet101]~\cite{siam2023textilenet} & 43.71 & 77.94 & 79.33 & 77.94 & 78.47 \\
Siam et al. [ConvNextBase]~\cite{siam2023textilenet} & 88.25 & 29.78 & 8.87 & 29.78 & 13.67 \\
Siam et al. [MobileNetV2]~\cite{siam2023textilenet} & 0.30 & 81.59 & 82.90 & 81.59 & 82.27 \\
\textbf{Proposed Method} & \textbf{28.97} & \textbf{74.36} & \textbf{72.73} & \textbf{73.36} & \textbf{72.38} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}